// crawler_new.yak - 使用 yak_fast 运行更快
// 后续 script/*.yak 漏洞检测仅使用 API 的 httpflow-id；给 AI 看输出时可过滤：2>&1 | grep -v '\[ERRO\]' | grep -v '\[WARN\]'
yakit.AutoInitYakit()

targetUrl = cli.String("targetUrl", cli.setHelp("爬虫目标url"),cli.setVerboseName("目标"),cli.setRequired(true))
wsAddress = cli.String("wsAddress", cli.setHelp("chrome headless运行时的ws地址"),cli.setVerboseName("浏览器ws地址"),cli.setCliGroup("浏览器"))
exePath = cli.String("exePath", cli.setHelp("chrome浏览器可执行程序的路径"),cli.setVerboseName("浏览器执行程序路径"),cli.setCliGroup("浏览器"))
proxy = cli.String("proxy", cli.setHelp("代理地址"),cli.setVerboseName("代理地址"),cli.setCliGroup("浏览器"))
proxyUsername = cli.String("proxyUsername", cli.setHelp("代理用户名"),cli.setVerboseName("代理用户名"),cli.setCliGroup("浏览器"))
proxyPassword = cli.String("proxyPassword", cli.setHelp("代理密码"),cli.setVerboseName("代理密码"),cli.setCliGroup("浏览器"))
pageTimeout = cli.Int("pageTimeout", cli.setDefault(15),cli.setHelp("单页面爬虫的最大操作时间，单位秒"),cli.setVerboseName("单页面超时时间"),cli.setCliGroup("爬虫参数"))
fullTimeout = cli.Int("fullTimeout", cli.setDefault(600),cli.setHelp("整个爬虫运行的超时时间 单位秒"),cli.setVerboseName("全局超时时间"),cli.setCliGroup("爬虫参数"))
formFill = cli.String("formFill", cli.setDefault("username:admin;password:admin"),cli.setHelp("key和value用英文冒号隔开，不同组数据用英文分号隔开"),cli.setVerboseName("表单填写"),cli.setCliGroup("爬虫参数"))
fileUpload = cli.String("fileUpload", cli.setDefault("default:/opt/defaultFile.txt;"),cli.setHelp("key（关键词）和value（文件路径）用英文冒号隔开，不同组数据用英文分号隔开\nkey为default时value为默认上传文件路径"),cli.setVerboseName("文件输入"),cli.setCliGroup("爬虫参数"))
header = cli.String("header", cli.setHelp("header名和header值用英文冒号隔开，不同组数据用英文分号隔开"),cli.setCliGroup("爬虫参数"))
cookie = cli.String("cookie", cli.setHelp("cookie名和cookie值用英文冒号隔开，不同组数据用英文分号隔开"),cli.setCliGroup("爬虫参数"))
scanRange = cli.StringSlice("scanRange", 
    cli.setMultipleSelect(false),
    cli.setSelectOption("全域名扫描", "AllDomainScan"),
    cli.setSelectOption("子域名扫描", "SubMenuScan"),
    cli.setSelectOption("无域名限制扫描", "UnlimitedScan"),
    cli.setVerboseName("扫描范围"),
    cli.setCliGroup("爬虫参数"),
    cli.setDefault("AllDomainScan")
)
scanRepeat = cli.StringSlice("scanRepeat", cli.setMultipleSelect(false),cli.setSelectOption("ExtremeRepeatLevel", "ExtremeRepeatLevel"),cli.setSelectOption("HighRepeatLevel", "HighRepeatLevel"),cli.setSelectOption("MediumRepeatLevel", "MediumRepeatLevel"),cli.setSelectOption("LowRepeatLevel", "LowRepeatLevel"),cli.setSelectOption("UnLimitRepeat", "UnLimitRepeat"),cli.setVerboseName("url去重级别"),cli.setCliGroup("爬虫参数"),cli.setDefault("HighRepeatLevel"))  // 改为高去重级别
maxUrl = cli.Int("maxUrl", cli.setVerboseName("最大url数量"),cli.setCliGroup("爬虫参数"))
maxDepth = cli.Int("maxDepth", cli.setVerboseName("最大爬虫深度"),cli.setCliGroup("爬虫参数"))
ignoreQuery = cli.String("ignoreQuery", cli.setHelp("url去重检测时忽略的query-name 以英文逗号隔开"),cli.setVerboseName("忽略参数名"),cli.setCliGroup("爬虫参数"))
extraWaitLoad = cli.Int("extraWaitLoad", cli.setHelp("页面加载的额外等待时间 单位毫秒"),cli.setVerboseName("额外等待时间"),cli.setCliGroup("爬虫参数"))

blacklist = cli.String("blacklist", cli.setHelp("url黑名单，以英文逗号隔开"),cli.setVerboseName("url黑名单"),cli.setCliGroup("爬虫参数"))
whitelist = cli.String("whitelist", cli.setHelp("url白名单，以英文逗号隔开"),cli.setVerboseName("url白名单"),cli.setCliGroup("爬虫参数"))
sensitiveWords = cli.String("sensitiveWords", cli.setHelp("当设置敏感词时，对应待操作元素innerHTml中存在该词汇则不会进行操作\n不同词之间用英文逗号隔开"),cli.setVerboseName("敏感词"),cli.setCliGroup("爬虫参数"))
leakless = cli.StringSlice("leakless", cli.setMultipleSelect(false),cli.setSelectOption("default", "default"),cli.setSelectOption("true", "true"),cli.setSelectOption("false", "false"),cli.setHelp("浏览器自动进程关闭进行在windows下会报病毒 默认在windows下会关闭\n当关闭时 如果强制关闭爬虫进程时chrome.exe会存在后台 过多时需要手动进行关闭"),cli.setVerboseName("浏览器进程自动关闭"),cli.setCliGroup("其他参数"),cli.setDefault("default"))
concurrent = cli.Int("concurrent", cli.setDefault(10),cli.setVerboseName("浏览器同时打开页面数量"),cli.setCliGroup("爬虫参数"))
showStatic = cli.Bool("showStatic", cli.setDefault(false),cli.setHelp("是否显示静态资源(js/css/图片等)"),cli.setVerboseName("显示静态资源"),cli.setCliGroup("其他参数"))
rawHeaders = cli.Text("rawHeaders", cli.setHelp("数据包中原始headers文本块"),cli.setCliGroup("爬虫参数"))
rawCookie = cli.Text("rawCookie", cli.setHelp("数据包中原始cookie文本块"),cli.setCliGroup("爬虫参数"))
cli.check()

func stringToDict(tempStr) {
    result = make(map[string]string, 0)
    items = tempStr.Split(";")
    for _, item := range items {
        if item.Contains(":") {
            kv := item.Split(":")
            result[kv[0]] = kv[1]
        }
    }
    return result
}

host = ""
if targetUrl.Contains("://") {
    host = targetUrl.Split("://")[1]
} else {
    host = targetUrl
}
host = host.Split("/")[0]

scanRangeMap = {
    "AllDomainScan": crawlerx.AllDomainScan,
    "SubMenuScan": crawlerx.SubMenuScan,
    "UnlimitedScan": crawlerx.UnlimitedDomainScan,
}

scanRepeatMap = {
    "UnLimitRepeat": crawlerx.UnLimitRepeat,
    "LowRepeatLevel": crawlerx.LowRepeatLevel,
    "MediumRepeatLevel": crawlerx.MediumRepeatLevel,
    "HighRepeatLevel": crawlerx.HighRepeatLevel,
    "ExtremeRepeatLevel": crawlerx.ExtremeRepeatLevel,
}

browserInfo = {
    "ws_address":"",
    "exe_path":"",
    "proxy_address":"",
    "proxy_username":"",
    "proxy_password":"",
}
if wsAddress != "" {
    browserInfo["ws_address"] = wsAddress
}
if exePath != "" {
    browserInfo["exe_path"] = exePath
}
if proxy != "" {
    browserInfo["proxy_address"] = proxy
    if proxyUsername != "" {
        browserInfo["proxy_username"] = proxyUsername
        browserInfo["proxy_password"] = proxyPassword
    }
}
browserInfoOpt = crawlerx.browserInfo(json.dumps(browserInfo))

pageTimeoutOpt = crawlerx.pageTimeout(pageTimeout)

// 小规模爬取时缩短全局超时，避免长时间等待
effectiveFullTimeout = fullTimeout
if maxUrl > 0 && maxUrl <= 100 && fullTimeout > 300 {
    effectiveFullTimeout = 300
}
fullTimeoutOpt = crawlerx.fullTimeout(effectiveFullTimeout)

concurrentOpt = crawlerx.concurrent(concurrent)

opts = [
    browserInfoOpt,
    pageTimeoutOpt,
    fullTimeoutOpt,
    concurrentOpt,
    crawlerx.sourceType("plugin"),
    crawlerx.fromPlugin("headless_crawlerx"),
    crawlerx.saveToDB(true),
]

if formFill != "" {
    formFillInfo = stringToDict(formFill)
    formFillOpt = crawlerx.formFill(formFillInfo)
    opts = append(opts, formFillOpt)
}

if fileUpload != "" {
    fileUploadInfo = stringToDict(fileUpload)
    fileUploadOpt = crawlerx.fileInput(fileUploadInfo)
    opts = append(opts, fileUploadOpt)
}

if header != "" {
    headerInfo = stringToDict(header)
    headerOpt = crawlerx.headers(headerInfo)
    opts = append(opts, headerOpt)
}

if rawHeaders != "" {
    opts = append(opts, crawlerx.rawHeaders(rawHeaders))
}

if rawCookie != "" {
    opts = append(opts, crawlerx.rawCookie(host, rawCookie))
}

if cookie != "" {
    cookieInfo = stringToDict(cookie)
    cookieOpt = crawlerx.cookies(host, cookieInfo)
    opts = append(opts, cookieOpt)
}

scanRangeStr = scanRange[0]
if scanRangeStr != "" {
    scanRangeItem = scanRangeMap[scanRangeStr]
    scanRangeOpt = crawlerx.scanRangeLevel(scanRangeItem)
    opts = append(opts, scanRangeOpt)
}

scanRepeatStr = scanRepeat[0]
if scanRepeatStr != "" {
    scanRepeatItem = scanRepeatMap[scanRepeatStr]
    scanRepeatOpt = crawlerx.scanRepeatLevel(scanRepeatItem)
    opts = append(opts, scanRepeatOpt)
}

if maxUrl != 0 {
    opts = append(opts, crawlerx.maxUrl(maxUrl))
}

if maxDepth != 0 {
    opts = append(opts, crawlerx.maxDepth(maxDepth))
}

if extraWaitLoad != 0 {
    opts = append(opts, crawlerx.extraWaitLoadTime(extraWaitLoad))
}

if ignoreQuery != "" {
    opts = append(opts, crawlerx.ignoreQueryName(ignoreQuery.Split(",")...))
}

if blacklist != "" {
    opts = append(opts, crawlerx.blacklist(blacklist.Split(",")...))
}

if whitelist != "" {
    opts = append(opts, crawlerx.whitelist(whitelist.Split(",")...))
}

if sensitiveWords != "" {
    opts = append(opts, crawlerx.sensitiveWords(sensitiveWords.Split(",")))
}

leaklessStr = leakless[0]
if leaklessStr != "" {
    opts = append(opts, crawlerx.leakless(leaklessStr))
}

// 使用 runtimeID 标识本次爬虫
runtimeId = uuid()
opts = append(opts, crawlerx.runtimeID(runtimeId))

// 运行爬虫，实时输出进度
startTime = time.Now().Unix()
yakit.Info("=== 爬虫开始 ===")
yakit.Info(sprintf("目标: %s", targetUrl))
yakit.Info(sprintf("并发: %d | 最大URL: %d | 单页超时: %ds | 全局超时: %ds", concurrent, maxUrl, pageTimeout, effectiveFullTimeout))
ch = crawlerx.StartCrawler(targetUrl, opts...)~
count = 0
progressStep = 5
if maxUrl > 50 || maxUrl == 0 {
    progressStep = 10
}
for item = range ch{
    count++
    if count % progressStep == 0 {
        elapsed = time.Now().Unix() - startTime
        yakit.Info(sprintf("已发现 %d 条请求 (耗时 %ds)...", count, elapsed))
    }
}
elapsed = time.Now().Unix() - startTime
yakit.Info(sprintf("=== 爬虫完成，共发现 %d 个请求，耗时 %ds ===", count, elapsed))

// 等待数据库写入
sleep(1)

// 静态资源后缀黑名单
staticExtensions = [
    ".js", ".css", ".jpg", ".jpeg", ".png", ".gif", ".svg", ".ico",
    ".woff", ".woff2", ".ttf", ".eot", ".otf",  // 字体
    ".mp4", ".mp3", ".avi", ".mov", ".webm",    // 媒体
    ".pdf", ".zip", ".rar", ".tar", ".gz",      // 文档/压缩包
    ".xml", ".json", ".map"                     // 数据文件
]

// 判断 URL 是否为静态资源
func isStaticResource(url) {
    for _, ext = range staticExtensions {
        if url.EndsWith(ext) || url.Contains(ext + "?") {
            return true
        }
    }
    // 额外判断常见静态路径
    if url.Contains("/assets/") || url.Contains("/static/") ||
       url.Contains("/public/") || url.Contains("/dist/") ||
       url.Contains("fonts.gstatic.com") {
        return true
    }
    return false
}

// 通过 runtimeId 查询本次爬虫的 HTTPFlow
// 后续流程（script/*.yak 漏洞检测）仅使用 API 的 httpflow-id
apiCount = 0
staticCount = 0
apiFlows = []
for flow = range db.QueryHTTPFlowsByKeyword(host) {
    if flow.RuntimeId == runtimeId {
        if isStaticResource(flow.Url) {
            staticCount++
        } else if flow.Url.Contains("/api/") || flow.Method != "GET" {
            apiFlows = append(apiFlows, flow)
            apiCount++
        }
    }
}
yakit.Info("=== API 列表（供 script/*.yak 使用）===")
for _, flow = range apiFlows {
    yakit.Info(sprintf("[ID:%d] [API] %s %s", flow.Model.ID, flow.Method, flow.Url))
}
if showStatic && staticCount > 0 {
    yakit.Info("=== 静态资源（已过滤）===")
    for flow = range db.QueryHTTPFlowsByKeyword(host) {
        if flow.RuntimeId == runtimeId && isStaticResource(flow.Url) {
            yakit.Info(sprintf("[ID:%d] [STATIC] %s %s", flow.Model.ID, flow.Method, flow.Url))
        }
    }
}
yakit.Info(sprintf("=== 统计: %d 个 API, %d 个静态(已过滤) ===", apiCount, staticCount))